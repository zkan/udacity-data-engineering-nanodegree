version: '3'

x-airflow-common:
  &airflow-common
  # In order to add custom dependencies or upgrade provider packages you can use your extended image.
  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml
  # and uncomment the "build" line below, Then run `docker-compose build` to build the images.
  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.2.4}
  build:
    context: .
    dockerfile: Dockerfile-spark
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKEND: 'airflow.api.auth.backend.basic_auth'
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
  volumes:
    - ./mnt/dags:/opt/airflow/dags
    - ./mnt/logs:/opt/airflow/logs
    - ./mnt/plugins:/opt/airflow/plugins
    - ./spark/app:/usr/local/spark/app #Spark Scripts (Must be the same path in airflow and Spark Cluster)
    #- /spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    &airflow-common-depends-on
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    restart: always

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - 8080:8080
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      #airflow-init:
      #  condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      #airflow-init:
      #  condition: service_completed_successfully

  #airflow-init:
  #  <<: *airflow-common
  #  entrypoint: /bin/bash
  #  # yamllint disable rule:line-length
  #  command:
  #    - -c
  #    - |
  #      function ver() {
  #        printf "%04d%04d%04d%04d" $${1//./ }
  #      }
  #      airflow_version=$$(gosu airflow airflow version)
  #      airflow_version_comparable=$$(ver $${airflow_version})
  #      min_airflow_version=2.2.0
  #      min_airflow_version_comparable=$$(ver $${min_airflow_version})
  #      if (( airflow_version_comparable < min_airflow_version_comparable )); then
  #        echo
  #        echo -e "\033[1;31mERROR!!!: Too old Airflow version $${airflow_version}!\e[0m"
  #        echo "The minimum Airflow version supported: $${min_airflow_version}. Only use this or higher!"
  #        echo
  #        exit 1
  #      fi
  #      if [[ -z "${AIRFLOW_UID}" ]]; then
  #        echo
  #        echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
  #        echo "If you are on Linux, you SHOULD follow the instructions below to set "
  #        echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
  #        echo "For other operating systems you can get rid of the warning with manually created .env file:"
  #        echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html#setting-the-right-airflow-user"
  #        echo
  #      fi
  #      one_meg=1048576
  #      mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
  #      cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
  #      disk_available=$$(df / | tail -1 | awk '{print $$4}')
  #      warning_resources="false"
  #      if (( mem_available < 4000 )) ; then
  #        echo
  #        echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
  #        echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
  #        echo
  #        warning_resources="true"
  #      fi
  #      if (( cpus_available < 2 )); then
  #        echo
  #        echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
  #        echo "At least 2 CPUs recommended. You have $${cpus_available}"
  #        echo
  #        warning_resources="true"
  #      fi
  #      if (( disk_available < one_meg * 10 )); then
  #        echo
  #        echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
  #        echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
  #        echo
  #        warning_resources="true"
  #      fi
  #      if [[ $${warning_resources} == "true" ]]; then
  #        echo
  #        echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
  #        echo "Please follow the instructions to increase amount of resources available:"
  #        echo "   https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html#before-you-begin"
  #        echo
  #      fi
  #      mkdir -p /sources/logs /sources/dags /sources/plugins
  #      chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}
  #      exec /entrypoint airflow version
  #  # yamllint enable rule:line-length
  #  environment:
  #    <<: *airflow-common-env
  #    _AIRFLOW_DB_UPGRADE: 'true'
  #    _AIRFLOW_WWW_USER_CREATE: 'true'
  #    _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
  #    _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
  #  user: "0:0"
  #  volumes:
  #    - .:/sources

  #airflow-cli:
  #  <<: *airflow-common
  #  profiles:
  #    - debug
  #  environment:
  #    <<: *airflow-common-env
  #    CONNECTION_CHECK_MAX_COUNT: "0"
  #  # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
  #  command:
  #    - bash
  #    - -c
  #    - airflow

  spark:
    image: bitnami/spark:3.2.1
    user: root # Run container as root container: https://docs.bitnami.com/tutorials/work-with-non-root-containers/
    hostname: spark
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    #volumes:
    #  - ./spark/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
    #  - ./spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
    ports:
      - "8181:8080"
      - "7077:7077"

  spark-worker:
    image: bitnami/spark:3.2.1
    user: root
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    #volumes:
    #  - ./spark/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
    #  - ./spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)

  #pyspark-notebook:
  #  image: jupyter/pyspark-notebook
  #  volumes:
  #    - ./notebooks:/home/jovyan/work
  #    #- ./spark/resources/data:/home/jovyan/work/data/
  #    #- ./spark/resources/jars:/home/jovyan/work/jars/
  #  ports:
  #    - 8888:8888
  #    - 4040:4040
  #    - 4041:4041
  #
  #minio:
  #  image: minio/minio:RELEASE.2022-01-08T03-11-54Z
  #  environment:
  #    MINIO_ROOT_USER: minio
  #    MINIO_ROOT_PASSWORD: minio123
  #  ports:
  #    - 9000:9000
  #    - 9001:9001
  #  volumes:
  #    - minio-data-volume:/data
  #  healthcheck:
  #    test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
  #    interval: 30s
  #    timeout: 20s
  #    retries: 3
  #  restart: always
  #  command: server /data --console-address ":9001"
  #
  #warehouse:
  #  image: postgres:13
  #  environment:
  #    - POSTGRES_USER=postgres
  #    - POSTGRES_PASSWORD=postgres
  #    - POSTGRES_DB=postgres
  #  volumes:
  #    - warehouse-db-volume:/var/lib/postgresql/data
  #  healthcheck:
  #    test: ["CMD", "pg_isready", "-q", "-d", "postgres", "-U", "postgres"]
  #    timeout: 30s
  #    interval: 5s
  #    retries: 5
  #  restart: always

volumes:
  postgres-db-volume:
  minio-data-volume:
  warehouse-db-volume: